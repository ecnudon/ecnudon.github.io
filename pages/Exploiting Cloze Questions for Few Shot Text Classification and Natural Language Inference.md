links:: [Local library](zotero://select/library/items/8WIHKXP2), [Web library](https://www.zotero.org/users/9034808/items/8WIHKXP2)
authors:: [[Timo Schick]], [[Hinrich Schütze]]
tags:: [[Computer Science - Computation and Language]]
date:: [[Mon, 2021/01/25]]
item-type:: [[preprint]]
title:: Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference

- [[Abstract]]
	- Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with "task descriptions" in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in low-resource settings by a large margin.
- [[Attachments]]
	- [arXiv.org Snapshot](https://arxiv.org/abs/2001.07676) {{zotero-imported-file EISBZFZU, "2001.html"}}
	- [Exploiting Cloze Questions for Few Shot Text Classification and Natural_2021_Schick_Schütze_.pdf](zotero://select/library/items/URZJ2H6J) {{zotero-linked-file "attachments:Few-shot/Exploiting Cloze Questions for Few Shot Text Classification and Natural_2021_Schick_Schütze_.pdf"}}
-
- [[note]]
	- x是输入的信息, PVP会把x加一个prompt  变成 P(x)  再看词表中的哪个词更适合mask处
	- 正常PET没法利用多种不一样的pv对,因此采用这样的方式:
		- 同样的labeled data, 不同的pv对上训练一个